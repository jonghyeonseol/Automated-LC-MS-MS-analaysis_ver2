"""
Regression Analyzer - ê³ ê¸‰ íšŒê·€ë¶„ì„ ì „ìš© ëª¨ë“ˆ
OLS íšŒê·€ë¶„ì„, ì”ì°¨ë¶„ì„, Durbin-Watson í…ŒìŠ¤íŠ¸, Cook's Distance ë“±
"""

import warnings
from typing import Any, Dict, List

import numpy as np
from scipy import stats

warnings.filterwarnings("ignore")


class RegressionAnalyzer:
    def __init__(self):
        self.r2_threshold = 0.99
        self.outlier_threshold = 3.0
        self.confidence_level = 0.95

        print("ğŸ“ˆ Regression Analyzer ì´ˆê¸°í™” ì™„ë£Œ")

    def perform_comprehensive_regression(
        self, x_data: np.ndarray, y_data: np.ndarray, compound_names: List[str] = None,
        feature_names: List[str] = None
    ) -> Dict[str, Any]:
        """
        ì¢…í•©ì ì¸ íšŒê·€ë¶„ì„ ìˆ˜í–‰ (ë‹¤ì¤‘íšŒê·€ ì§€ì›)

        Args:
            x_data: ë…ë¦½ë³€ìˆ˜ (1D: Log Pë§Œ ë˜ëŠ” 2D: ì—¬ëŸ¬ íŠ¹ì„±)
            y_data: ì¢…ì†ë³€ìˆ˜ (RT)
            compound_names: í™”í•©ë¬¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            feature_names: íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (e.g., ['Log P', 'Carbon', 'Sugar'])

        Returns:
            ì¢…í•© íšŒê·€ë¶„ì„ ê²°ê³¼
        """

        # x_dataë¥¼ 2D ë°°ì—´ë¡œ ë³€í™˜
        if len(x_data.shape) == 1:
            x_data = x_data.reshape(-1, 1)

        # ê¸°ë³¸ feature_names ì„¤ì •
        if feature_names is None:
            if x_data.shape[1] == 1:
                feature_names = ['Log P']
            else:
                feature_names = [f'Feature_{i+1}' for i in range(x_data.shape[1])]

        if len(x_data) < 3:
            return self._minimal_regression_result(x_data, y_data, feature_names)

        try:
            # 1. ê¸°ë³¸ OLS íšŒê·€ë¶„ì„
            basic_regression = self._perform_ols_regression(x_data, y_data, feature_names)

            # 2. ì”ì°¨ ì§„ë‹¨
            residual_diagnostics = self._comprehensive_residual_analysis(
                basic_regression["residuals"], basic_regression["predicted_values"]
            )

            # 3. ì˜í–¥ë ¥ ì§„ë‹¨ (Cook's Distance, Leverage, DFFITS)
            influence_diagnostics = self._influence_diagnostics(
                x_data, y_data, basic_regression
            )

            # 4. ê³ ê¸‰ ì´ìƒì¹˜ íƒì§€
            outlier_analysis = self._advanced_outlier_detection(
                x_data, y_data, basic_regression, compound_names
            )

            # 5. ëª¨ë¸ ê°€ì • ê²€ì •
            assumption_tests = self._test_regression_assumptions(
                basic_regression["residuals"],
                basic_regression["predicted_values"],
                x_data,
            )

            # 6. ì˜ˆì¸¡ êµ¬ê°„ ê³„ì‚°
            prediction_intervals = self._calculate_prediction_intervals(
                x_data, y_data, basic_regression
            )

            # 7. ëª¨ë¸ í’ˆì§ˆ í‰ê°€
            model_quality = self._evaluate_model_quality(
                basic_regression, residual_diagnostics, assumption_tests
            )

            return {
                "basic_regression": basic_regression,
                "residual_diagnostics": residual_diagnostics,
                "influence_diagnostics": influence_diagnostics,
                "outlier_analysis": outlier_analysis,
                "assumption_tests": assumption_tests,
                "prediction_intervals": prediction_intervals,
                "model_quality": model_quality,
                "feature_names": feature_names,
                "n_features": x_data.shape[1],
                "analysis_summary": self._generate_analysis_summary(
                    basic_regression, model_quality, outlier_analysis
                ),
            }

        except Exception as e:
            print(f"Regression analysis error: {str(e)}")
            return self._error_regression_result(str(e))

    def _perform_ols_regression(
        self, x_data: np.ndarray, y_data: np.ndarray, feature_names: List[str] = None
    ) -> Dict[str, Any]:
        """ê¸°ë³¸ OLS íšŒê·€ë¶„ì„ ìˆ˜í–‰ (ë‹¤ì¤‘íšŒê·€ ì§€ì›)"""

        n = len(x_data)

        # x_dataê°€ 1Dë©´ 2Dë¡œ ë³€í™˜
        if len(x_data.shape) == 1:
            x_data = x_data.reshape(-1, 1)

        n_features = x_data.shape[1]

        # ì„¤ê³„ í–‰ë ¬ ìƒì„± (ì ˆí¸ í¬í•¨)
        X = np.column_stack([np.ones(n), x_data])

        # OLS ì¶”ì •
        try:
            # ì •ê·œë°©ì •ì‹: Î² = (X'X)^(-1)X'y
            XTX_inv = np.linalg.inv(X.T @ X)
            beta = XTX_inv @ X.T @ y_data
        except np.linalg.LinAlgError:
            # íŠ¹ì´í–‰ë ¬ì¸ ê²½ìš° ìµœì†Œì œê³±ë²• ì‚¬ìš©
            beta, _, _, _ = np.linalg.lstsq(X, y_data, rcond=None)

        intercept = beta[0]
        coefficients = beta[1:]  # ê° íŠ¹ì„±ì˜ ê³„ìˆ˜

        # ì˜ˆì¸¡ê°’ ë° ì”ì°¨ ê³„ì‚°
        y_pred = X @ beta
        residuals = y_data - y_pred

        # í†µê³„ëŸ‰ ê³„ì‚°
        ss_total = np.sum((y_data - np.mean(y_data)) ** 2)
        ss_residual = np.sum(residuals**2)
        ss_regression = ss_total - ss_residual

        r2 = ss_regression / ss_total if ss_total > 0 else 0

        # ììœ ë„: n - (p + 1), where p is number of features
        df = n - (n_features + 1)
        adjusted_r2 = (
            1 - (ss_residual / df) / (ss_total / (n - 1)) if df > 0 and n > 1 else r2
        )

        # í‘œì¤€ì˜¤ì°¨ ê³„ì‚°
        mse = ss_residual / df if df > 0 else 0

        # ê° ê³„ìˆ˜ì˜ í‘œì¤€ì˜¤ì°¨
        se_coefficients = []
        t_statistics = []
        p_values = []

        for i in range(len(beta)):
            se = np.sqrt(mse * XTX_inv[i, i]) if mse > 0 else 0
            t_stat = beta[i] / se if se > 0 else 0
            p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df)) if df > 0 else 0.5

            se_coefficients.append(float(se))
            t_statistics.append(float(t_stat))
            p_values.append(float(p_val))

        # F-í†µê³„ëŸ‰ (ì „ì²´ ëª¨ë¸ ìœ ì˜ì„±)
        f_statistic = (
            (ss_regression / n_features) / (ss_residual / df)
            if df > 0 and ss_residual > 0
            else 0
        )
        f_p_value = 1 - stats.f.cdf(f_statistic, n_features, df) if df > 0 else 0.5

        # íšŒê·€ì‹ ìƒì„±
        if feature_names is None:
            feature_names = [f'X{i+1}' for i in range(n_features)]

        equation_parts = [f"{intercept:.4f}"]
        for i, (coef, name) in enumerate(zip(coefficients, feature_names)):
            sign = "+" if coef >= 0 else "-"
            equation_parts.append(f"{sign} {abs(coef):.4f}*{name}")
        equation = f"RT = {' '.join(equation_parts)}"

        # ê³„ìˆ˜ ìƒì„¸ ì •ë³´
        coefficient_details = {}
        for i, name in enumerate(feature_names):
            coefficient_details[name] = {
                "coefficient": float(coefficients[i]),
                "std_error": se_coefficients[i+1],  # +1 for intercept
                "t_statistic": t_statistics[i+1],
                "p_value": p_values[i+1],
                "significant": p_values[i+1] < 0.05
            }

        return {
            "intercept": float(intercept),
            "coefficients": coefficients.tolist(),
            "coefficient_details": coefficient_details,
            "r2": float(r2),
            "adjusted_r2": float(adjusted_r2),
            "mse": float(mse),
            "rmse": float(np.sqrt(mse)),
            "se_intercept": se_coefficients[0],
            "t_intercept": t_statistics[0],
            "p_intercept": p_values[0],
            "f_statistic": float(f_statistic),
            "f_p_value": float(f_p_value),
            "predicted_values": y_pred.tolist(),
            "residuals": residuals.tolist(),
            "standardized_residuals": (residuals / np.std(residuals)).tolist()
            if np.std(residuals) > 0
            else residuals.tolist(),
            "n_observations": n,
            "n_features": n_features,
            "degrees_freedom": df,
            "equation": equation,
            # Legacy fields for backward compatibility
            "slope": float(coefficients[0]) if n_features == 1 else float(coefficients[0]),
            "se_slope": se_coefficients[1] if n_features >= 1 else 0,
            "t_slope": t_statistics[1] if n_features >= 1 else 0,
            "p_slope": p_values[1] if n_features >= 1 else 0,
        }

    def _comprehensive_residual_analysis(
        self, residuals: List[float], predicted_values: List[float]
    ) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ ì”ì°¨ ë¶„ì„"""

        residuals = np.array(residuals)
        predicted_values = np.array(predicted_values)

        # 1. Durbin-Watson ê²€ì • (ìê¸°ìƒê´€)
        dw_statistic = self._durbin_watson_test(residuals)

        # 2. Breusch-Pagan ê²€ì • (ë“±ë¶„ì‚°ì„±)
        bp_test = self._breusch_pagan_test(residuals, predicted_values)

        # 3. Shapiro-Wilk ê²€ì • (ì •ê·œì„±)
        shapiro_test = self._shapiro_wilk_test(residuals)

        # 4. ì”ì°¨ í†µê³„
        residual_stats = {
            "mean": float(np.mean(residuals)),
            "std": float(np.std(residuals)),
            "min": float(np.min(residuals)),
            "max": float(np.max(residuals)),
            "range": float(np.max(residuals) - np.min(residuals)),
            "skewness": float(stats.skew(residuals)),
            "kurtosis": float(stats.kurtosis(residuals)),
        }

        return {
            "durbin_watson": dw_statistic,
            "breusch_pagan": bp_test,
            "shapiro_wilk": shapiro_test,
            "residual_statistics": residual_stats,
            "autocorrelation_detected": dw_statistic["is_problematic"],
            "heteroscedasticity_detected": bp_test["is_heteroscedastic"],
            "non_normality_detected": not shapiro_test["is_normal"],
        }

    def _durbin_watson_test(self, residuals: np.ndarray) -> Dict[str, Any]:
        """Durbin-Watson ê²€ì • (1ì°¨ ìê¸°ìƒê´€ ê²€ì •)"""

        n = len(residuals)
        if n < 2:
            return {
                "statistic": 2.0,
                "interpretation": "ë°ì´í„° ë¶€ì¡±",
                "is_problematic": False,
            }

        # DW í†µê³„ëŸ‰ ê³„ì‚°
        diff_residuals = np.diff(residuals)
        dw_stat = np.sum(diff_residuals**2) / np.sum(residuals**2)

        # í•´ì„ (ê°„ëµí™”ëœ ê¸°ì¤€)
        if dw_stat < 1.5:
            interpretation = "ì–‘ì˜ ìê¸°ìƒê´€ ì˜ì‹¬"
            is_problematic = True
        elif dw_stat > 2.5:
            interpretation = "ìŒì˜ ìê¸°ìƒê´€ ì˜ì‹¬"
            is_problematic = True
        else:
            interpretation = "ìê¸°ìƒê´€ ì—†ìŒ"
            is_problematic = False

        return {
            "statistic": float(dw_stat),
            "interpretation": interpretation,
            "is_problematic": is_problematic,
            "critical_range": [1.5, 2.5],
        }

    def _breusch_pagan_test(
        self, residuals: np.ndarray, predicted_values: np.ndarray
    ) -> Dict[str, Any]:
        """Breusch-Pagan ê²€ì • (ë“±ë¶„ì‚°ì„± ê²€ì •)"""

        n = len(residuals)
        if n < 3:
            return {"is_heteroscedastic": False, "p_value": 0.5, "test_statistic": 0.0}

        # ì”ì°¨ ì œê³±ê³¼ ì˜ˆì¸¡ê°’ì˜ ìƒê´€ê³„ìˆ˜
        squared_residuals = residuals**2

        try:
            correlation, p_value = stats.pearsonr(squared_residuals, predicted_values)

            # LM í†µê³„ëŸ‰ (ê°„ëµí™”)
            lm_stat = n * correlation**2

            # ì„ê³„ê°’ ì„¤ì • (ê°„ëµí™”)
            is_heteroscedastic = abs(correlation) > 0.3 or p_value < 0.05

            return {
                "correlation": float(correlation),
                "p_value": float(p_value),
                "lm_statistic": float(lm_stat),
                "is_heteroscedastic": is_heteroscedastic,
                "interpretation": "ì´ë¶„ì‚°ì„± ë°œê²¬" if is_heteroscedastic else "ë“±ë¶„ì‚°ì„± ê°€ì • ë§Œì¡±",
            }
        except:
            return {"is_heteroscedastic": False, "p_value": 0.5, "test_statistic": 0.0}

    def _shapiro_wilk_test(self, residuals: np.ndarray) -> Dict[str, Any]:
        """Shapiro-Wilk ê²€ì • (ì •ê·œì„± ê²€ì •)"""

        n = len(residuals)
        if n < 3:
            return {"is_normal": True, "p_value": 0.5, "statistic": 1.0}

        try:
            statistic, p_value = stats.shapiro(residuals)
            is_normal = p_value > 0.05

            return {
                "statistic": float(statistic),
                "p_value": float(p_value),
                "is_normal": is_normal,
                "interpretation": "ì •ê·œì„± ë§Œì¡±" if is_normal else "ì •ê·œì„± ìœ„ë°˜",
            }
        except:
            return {"is_normal": True, "p_value": 0.5, "statistic": 1.0}

    def _influence_diagnostics(
        self, x_data: np.ndarray, y_data: np.ndarray, regression_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì˜í–¥ë ¥ ì§„ë‹¨ (Cook's Distance, Leverage, DFFITS)"""

        n = len(x_data)

        # x_dataê°€ 1Dë©´ 2Dë¡œ ë³€í™˜
        if len(x_data.shape) == 1:
            x_data = x_data.reshape(-1, 1)

        n_features = x_data.shape[1]
        p = n_features + 1  # íŒŒë¼ë¯¸í„° ê°œìˆ˜ (ì ˆí¸ + íŠ¹ì„±ë“¤)

        # ì„¤ê³„ í–‰ë ¬
        X = np.column_stack([np.ones(n), x_data])

        try:
            # Hat matrix (H = X(X'X)^(-1)X')
            XTX_inv = np.linalg.inv(X.T @ X)
            H = X @ XTX_inv @ X.T
            leverage = np.diag(H)

            # ì”ì°¨
            residuals = np.array(regression_result["residuals"])
            mse = regression_result["mse"]

            # Cook's Distance
            standardized_residuals = residuals / np.sqrt(mse * (1 - leverage))
            cooks_distance = (standardized_residuals**2 / p) * (
                leverage / (1 - leverage)
            )

            # DFFITS
            dffits = standardized_residuals * np.sqrt(leverage / (1 - leverage))

            # ì„ê³„ê°’
            leverage_threshold = 2 * p / n
            cooks_threshold = 4 / (n - p)
            dffits_threshold = 2 * np.sqrt(p / n)

            # ì˜í–¥ë ¥ ìˆëŠ” ê´€ì¸¡ì¹˜ íƒì§€
            high_leverage = leverage > leverage_threshold
            high_cooks = cooks_distance > cooks_threshold
            high_dffits = np.abs(dffits) > dffits_threshold

            influential = high_leverage | high_cooks | high_dffits

            return {
                "leverage": leverage.tolist(),
                "cooks_distance": cooks_distance.tolist(),
                "dffits": dffits.tolist(),
                "thresholds": {
                    "leverage": float(leverage_threshold),
                    "cooks_distance": float(cooks_threshold),
                    "dffits": float(dffits_threshold),
                },
                "influential_points": {
                    "high_leverage": np.where(high_leverage)[0].tolist(),
                    "high_cooks": np.where(high_cooks)[0].tolist(),
                    "high_dffits": np.where(high_dffits)[0].tolist(),
                    "any_influential": np.where(influential)[0].tolist(),
                },
                "influence_summary": {
                    "n_high_leverage": int(np.sum(high_leverage)),
                    "n_high_cooks": int(np.sum(high_cooks)),
                    "n_high_dffits": int(np.sum(high_dffits)),
                    "n_influential": int(np.sum(influential)),
                },
            }

        except Exception as e:
            print(f"Influence diagnostics error: {str(e)}")
            return self._default_influence_result(n)

    def _advanced_outlier_detection(
        self,
        x_data: np.ndarray,
        y_data: np.ndarray,
        regression_result: Dict[str, Any],
        compound_names: List[str] = None,
    ) -> Dict[str, Any]:
        """ê³ ê¸‰ ì´ìƒì¹˜ íƒì§€"""

        n = len(x_data)
        residuals = np.array(regression_result["residuals"])
        std_residuals = np.array(regression_result["standardized_residuals"])

        # 1. í‘œì¤€í™” ì”ì°¨ ê¸°ë°˜ ì´ìƒì¹˜ (Â±3 ê¸°ì¤€)
        std_outliers = np.abs(std_residuals) >= self.outlier_threshold

        # 2. IQR ê¸°ë°˜ ì´ìƒì¹˜
        q1, q3 = np.percentile(residuals, [25, 75])
        iqr = q3 - q1
        iqr_outliers = (residuals < q1 - 1.5 * iqr) | (residuals > q3 + 1.5 * iqr)

        # 3. Modified Z-score (MAD ê¸°ë°˜)
        mad = np.median(np.abs(residuals - np.median(residuals)))
        modified_z_scores = (
            0.6745 * (residuals - np.median(residuals)) / mad
            if mad > 0
            else np.zeros_like(residuals)
        )
        mad_outliers = np.abs(modified_z_scores) > 3.5

        # 4. í†µí•© ì´ìƒì¹˜ íŒë³„
        combined_outliers = std_outliers | iqr_outliers | mad_outliers

        # ì´ìƒì¹˜ ìƒì„¸ ì •ë³´
        outlier_details = []
        for i in range(n):
            if combined_outliers[i]:
                outlier_info = {
                    "index": int(i),
                    "name": compound_names[i]
                    if compound_names and i < len(compound_names)
                    else f"Point_{i}",
                    "x_value": float(x_data[i]),
                    "y_value": float(y_data[i]),
                    "residual": float(residuals[i]),
                    "std_residual": float(std_residuals[i]),
                    "modified_z_score": float(modified_z_scores[i]),
                    "outlier_methods": [],
                }

                if std_outliers[i]:
                    outlier_info["outlier_methods"].append("Standardized Residual")
                if iqr_outliers[i]:
                    outlier_info["outlier_methods"].append("IQR Method")
                if mad_outliers[i]:
                    outlier_info["outlier_methods"].append("MAD Method")

                outlier_details.append(outlier_info)

        return {
            "outlier_indices": np.where(combined_outliers)[0].tolist(),
            "outlier_details": outlier_details,
            "outlier_summary": {
                "n_std_outliers": int(np.sum(std_outliers)),
                "n_iqr_outliers": int(np.sum(iqr_outliers)),
                "n_mad_outliers": int(np.sum(mad_outliers)),
                "n_total_outliers": int(np.sum(combined_outliers)),
                "outlier_percentage": float(np.sum(combined_outliers) / n * 100),
            },
            "thresholds": {
                "standardized_residual": self.outlier_threshold,
                "iqr_multiplier": 1.5,
                "mad_threshold": 3.5,
            },
        }

    def _test_regression_assumptions(
        self, residuals: List[float], predicted_values: List[float], x_data: np.ndarray
    ) -> Dict[str, Any]:
        """íšŒê·€ë¶„ì„ ê°€ì • ê²€ì •"""

        residuals = np.array(residuals)
        predicted_values = np.array(predicted_values)

        assumptions = {}

        # 1. ì„ í˜•ì„± (ì”ì°¨ vs ì˜ˆì¸¡ê°’ì˜ íŒ¨í„´)
        linearity_corr = (
            np.corrcoef(predicted_values, residuals)[0, 1] if len(residuals) > 1 else 0
        )
        assumptions["linearity"] = {
            "correlation": float(linearity_corr),
            "is_satisfied": abs(linearity_corr) < 0.1,
            "interpretation": "ì„ í˜•ì„± ë§Œì¡±" if abs(linearity_corr) < 0.1 else "ì„ í˜•ì„± ì˜ì‹¬",
        }

        # 2. ë…ë¦½ì„± (Durbin-Watson)
        dw_result = self._durbin_watson_test(residuals)
        assumptions["independence"] = {
            "durbin_watson": dw_result["statistic"],
            "is_satisfied": not dw_result["is_problematic"],
            "interpretation": dw_result["interpretation"],
        }

        # 3. ë“±ë¶„ì‚°ì„± (Breusch-Pagan)
        bp_result = self._breusch_pagan_test(residuals, predicted_values)
        assumptions["homoscedasticity"] = {
            "is_satisfied": not bp_result["is_heteroscedastic"],
            "test_result": bp_result,
            "interpretation": bp_result["interpretation"],
        }

        # 4. ì •ê·œì„± (Shapiro-Wilk)
        sw_result = self._shapiro_wilk_test(residuals)
        assumptions["normality"] = {
            "is_satisfied": sw_result["is_normal"],
            "test_result": sw_result,
            "interpretation": sw_result["interpretation"],
        }

        # ì¢…í•© í‰ê°€
        all_satisfied = all(
            [
                assumptions["linearity"]["is_satisfied"],
                assumptions["independence"]["is_satisfied"],
                assumptions["homoscedasticity"]["is_satisfied"],
                assumptions["normality"]["is_satisfied"],
            ]
        )

        assumptions["overall_assessment"] = {
            "all_assumptions_met": all_satisfied,
            "violated_assumptions": [
                name
                for name, test in assumptions.items()
                if isinstance(test, dict) and not test.get("is_satisfied", True)
            ],
        }

        return assumptions

    def _calculate_prediction_intervals(
        self, x_data: np.ndarray, y_data: np.ndarray, regression_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ì˜ˆì¸¡ êµ¬ê°„ ê³„ì‚°"""

        n = len(x_data)
        if n < 3:
            return {"confidence_intervals": [], "prediction_intervals": []}

        # x_dataê°€ 1Dë©´ 2Dë¡œ ë³€í™˜
        if len(x_data.shape) == 1:
            x_data = x_data.reshape(-1, 1)

        n_features = x_data.shape[1]
        intercept = regression_result["intercept"]
        coefficients = regression_result.get("coefficients", [regression_result.get("slope", 0)])
        mse = regression_result["mse"]
        df = regression_result["degrees_freedom"]

        # t ì„ê³„ê°’
        alpha = 1 - self.confidence_level
        t_critical = stats.t.ppf(1 - alpha / 2, df) if df > 0 else 1.96

        # ì„¤ê³„ í–‰ë ¬
        X = np.column_stack([np.ones(n), x_data])

        try:
            XTX_inv = np.linalg.inv(X.T @ X)

            confidence_intervals = []
            prediction_intervals = []

            for i in range(n):
                # ì˜ˆì¸¡ê°’ ê³„ì‚° (ë‹¤ì¤‘ íŠ¹ì„± ì§€ì›)
                x_row = x_data[i]
                if n_features == 1:
                    y_pred = intercept + coefficients[0] * x_row[0]
                else:
                    y_pred = intercept + np.sum(np.array(coefficients) * x_row)

                # í‘œì¤€ì˜¤ì°¨
                x_vector = np.concatenate([[1], x_row])
                se_pred = np.sqrt(mse * x_vector.T @ XTX_inv @ x_vector)
                se_forecast = np.sqrt(mse * (1 + x_vector.T @ XTX_inv @ x_vector))

                # ì‹ ë¢°êµ¬ê°„ (í‰ê·  ì˜ˆì¸¡)
                ci_lower = y_pred - t_critical * se_pred
                ci_upper = y_pred + t_critical * se_pred

                # ì˜ˆì¸¡êµ¬ê°„ (ê°œë³„ ì˜ˆì¸¡)
                pi_lower = y_pred - t_critical * se_forecast
                pi_upper = y_pred + t_critical * se_forecast

                confidence_intervals.append(
                    {
                        "index": i,
                        "predicted": float(y_pred),
                        "lower": float(ci_lower),
                        "upper": float(ci_upper),
                    }
                )

                prediction_intervals.append(
                    {
                        "index": i,
                        "predicted": float(y_pred),
                        "lower": float(pi_lower),
                        "upper": float(pi_upper),
                    }
                )

            return {
                "confidence_level": self.confidence_level,
                "confidence_intervals": confidence_intervals,
                "prediction_intervals": prediction_intervals,
            }

        except Exception as e:
            print(f"Prediction interval calculation error: {str(e)}")
            return {"confidence_intervals": [], "prediction_intervals": []}

    def _evaluate_model_quality(
        self,
        basic_regression: Dict[str, Any],
        residual_diagnostics: Dict[str, Any],
        assumption_tests: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ëª¨ë¸ í’ˆì§ˆ ì¢…í•© í‰ê°€"""

        r2 = basic_regression["r2"]
        adjusted_r2 = basic_regression["adjusted_r2"]
        f_p_value = basic_regression["f_p_value"]

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)
        quality_scores = {}

        # 1. ì„¤ëª…ë ¥ ì ìˆ˜ (RÂ² ê¸°ë°˜)
        quality_scores["explanatory_power"] = min(r2 * 100, 100)

        # 2. í†µê³„ì  ìœ ì˜ì„± ì ìˆ˜
        quality_scores["statistical_significance"] = (
            100 if f_p_value < 0.05 else 50 if f_p_value < 0.1 else 0
        )

        # 3. ê°€ì • ë§Œì¡±ë„ ì ìˆ˜
        assumptions_met = sum(
            [
                assumption_tests["linearity"]["is_satisfied"],
                assumption_tests["independence"]["is_satisfied"],
                assumption_tests["homoscedasticity"]["is_satisfied"],
                assumption_tests["normality"]["is_satisfied"],
            ]
        )
        quality_scores["assumption_compliance"] = (assumptions_met / 4) * 100

        # 4. ì”ì°¨ í’ˆì§ˆ ì ìˆ˜
        residual_quality = 100
        if residual_diagnostics["autocorrelation_detected"]:
            residual_quality -= 30
        if residual_diagnostics["heteroscedasticity_detected"]:
            residual_quality -= 25
        if residual_diagnostics["non_normality_detected"]:
            residual_quality -= 20
        quality_scores["residual_quality"] = max(residual_quality, 0)

        # ì¢…í•© ì ìˆ˜
        overall_score = np.mean(list(quality_scores.values()))

        # ë“±ê¸‰ íŒì •
        if overall_score >= 90:
            grade = "Excellent"
            reliability = "Very High"
        elif overall_score >= 80:
            grade = "Good"
            reliability = "High"
        elif overall_score >= 70:
            grade = "Acceptable"
            reliability = "Medium"
        elif overall_score >= 60:
            grade = "Poor"
            reliability = "Low"
        else:
            grade = "Unacceptable"
            reliability = "Very Low"

        return {
            "overall_score": float(overall_score),
            "grade": grade,
            "reliability": reliability,
            "quality_scores": {k: float(v) for k, v in quality_scores.items()},
            "meets_threshold": r2 >= self.r2_threshold,
            "recommendations": self._generate_recommendations(
                basic_regression, residual_diagnostics, assumption_tests
            ),
        }

    def _generate_recommendations(
        self,
        basic_regression: Dict[str, Any],
        residual_diagnostics: Dict[str, Any],
        assumption_tests: Dict[str, Any],
    ) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""

        recommendations = []

        # RÂ² ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        r2 = basic_regression["r2"]
        if r2 < 0.5:
            recommendations.append("ì„¤ëª…ë ¥ì´ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ ë³€ìˆ˜ë‚˜ ë¹„ì„ í˜• ëª¨ë¸ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        elif r2 < self.r2_threshold:
            recommendations.append("ì„¤ëª…ë ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì´ìƒì¹˜ ì œê±°ë‚˜ ë³€ìˆ˜ ë³€í™˜ì„ ê²€í† í•´ë³´ì„¸ìš”.")

        # ê°€ì • ìœ„ë°˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if not assumption_tests["linearity"]["is_satisfied"]:
            recommendations.append("ì„ í˜•ì„± ê°€ì •ì´ ìœ„ë°˜ë©ë‹ˆë‹¤. ë¹„ì„ í˜• íšŒê·€ë‚˜ ë³€ìˆ˜ ë³€í™˜ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")

        if not assumption_tests["independence"]["is_satisfied"]:
            recommendations.append("ë…ë¦½ì„± ê°€ì •ì´ ìœ„ë°˜ë©ë‹ˆë‹¤. ì‹œê³„ì—´ ëª¨ë¸ì´ë‚˜ ì¼ë°˜í™” ì„ í˜•ëª¨ë¸ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")

        if not assumption_tests["homoscedasticity"]["is_satisfied"]:
            recommendations.append("ë“±ë¶„ì‚°ì„± ê°€ì •ì´ ìœ„ë°˜ë©ë‹ˆë‹¤. ê°€ì¤‘ìµœì†Œì œê³±ë²•ì´ë‚˜ ë¡œë²„ìŠ¤íŠ¸ íšŒê·€ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")

        if not assumption_tests["normality"]["is_satisfied"]:
            recommendations.append("ì •ê·œì„± ê°€ì •ì´ ìœ„ë°˜ë©ë‹ˆë‹¤. ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì´ë‚˜ ë³€ìˆ˜ ë³€í™˜ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.")

        # ì´ìƒì¹˜ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if residual_diagnostics.get("outlier_detected", False):
            recommendations.append("ì´ìƒì¹˜ê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•˜ê³  ì´ìƒì¹˜ ì œê±°ë¥¼ ê²€í† í•´ë³´ì„¸ìš”.")

        if not recommendations:
            recommendations.append("íšŒê·€ëª¨ë¸ì´ ì–‘í˜¸í•œ í’ˆì§ˆì„ ë³´ì…ë‹ˆë‹¤. í˜„ì¬ ì„¤ì •ì„ ìœ ì§€í•˜ì„¸ìš”.")

        return recommendations

    def _generate_analysis_summary(
        self,
        basic_regression: Dict[str, Any],
        model_quality: Dict[str, Any],
        outlier_analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì•½ ìƒì„±"""

        return {
            "model_equation": basic_regression["equation"],
            "r_squared": basic_regression["r2"],
            "adjusted_r_squared": basic_regression["adjusted_r2"],
            "p_value": basic_regression["f_p_value"],
            "sample_size": basic_regression["n_observations"],
            "quality_grade": model_quality["grade"],
            "overall_score": model_quality["overall_score"],
            "outliers_detected": outlier_analysis["outlier_summary"][
                "n_total_outliers"
            ],
            "outlier_percentage": outlier_analysis["outlier_summary"][
                "outlier_percentage"
            ],
            "meets_quality_threshold": model_quality["meets_threshold"],
            "key_recommendations": model_quality["recommendations"][:3],  # ìƒìœ„ 3ê°œ ê¶Œì¥ì‚¬í•­
        }

    def _minimal_regression_result(
        self, x_data: np.ndarray, y_data: np.ndarray, feature_names: List[str] = None
    ) -> Dict[str, Any]:
        """ìµœì†Œ ë°ì´í„°ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ íšŒê·€ ê²°ê³¼"""

        if len(x_data) == 0:
            return self._empty_regression_result()

        basic_regression = self._perform_ols_regression(x_data, y_data, feature_names)

        return {
            "basic_regression": basic_regression,
            "model_quality": {
                "overall_score": 50.0,
                "grade": "Insufficient Data",
                "reliability": "Low",
                "meets_threshold": False,
            },
            "analysis_summary": {
                "model_equation": basic_regression["equation"],
                "r_squared": basic_regression["r2"],
                "sample_size": len(x_data),
                "key_recommendations": ["ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."],
            },
        }

    def _empty_regression_result(self) -> Dict[str, Any]:
        """ë¹ˆ ë°ì´í„°ë¥¼ ìœ„í•œ ê¸°ë³¸ ê²°ê³¼"""

        return {
            "basic_regression": {"equation": "RT = 0.0000 * Log_P + 0.0000", "r2": 0.0},
            "model_quality": {
                "overall_score": 0.0,
                "grade": "No Data",
                "reliability": "None",
            },
            "analysis_summary": {"key_recommendations": ["ë°ì´í„°ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."]},
        }

    def _error_regression_result(self, error_message: str) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê²°ê³¼"""

        return {
            "error": True,
            "error_message": error_message,
            "basic_regression": {"equation": "Error in calculation", "r2": 0.0},
            "model_quality": {
                "overall_score": 0.0,
                "grade": "Error",
                "reliability": "None",
            },
            "analysis_summary": {"key_recommendations": ["ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."]},
        }

    def _default_influence_result(self, n: int) -> Dict[str, Any]:
        """ì˜í–¥ë ¥ ì§„ë‹¨ ê¸°ë³¸ ê²°ê³¼"""

        return {
            "leverage": [0.0] * n,
            "cooks_distance": [0.0] * n,
            "dffits": [0.0] * n,
            "influential_points": {"any_influential": []},
            "influence_summary": {"n_influential": 0},
        }
